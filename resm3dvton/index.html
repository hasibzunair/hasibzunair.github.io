<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Res-M3D-VTON</title>
	<meta property="og:image" content="./resources/model.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Monocular-to-3D Virtual Try-On using Deep Residual U-Net" />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Monocular-to-3D Virtual Try-On using Deep Residual U-Net</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<!--Add author names here.-->
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="http://hasibzunair.github.io/">Hasib Zunair</a><sup>1</sup></span>
						</center>
					</td>
					
					<!--
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Second Author</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Third Author</a></span>
						</center>
					</td>
					-->
				</tr>
			</table>

			<!--Add affiliations here.-->
			<table align="center" width="700px">
			   <tbody><tr>
				<td align="center" width="200px">
				<center>
				  <span style="font-size:20px"><sup>1</sup>Concordia University, Montreal, QC, Canada.</span>
				</center>

				<!--
				</td>
				  <td align="center" width="200px">
				<center>
				  <span style="font-size:20px"><sup>2</sup>University X</span>
				</center>
				-->
				</td>
			 </tr>
			</tbody></table>

			
			<!--Add publication details here.-->
			<br>
			<table align="center" width="700px">
			<tbody><tr>
				<td align="center" width="100px">
				<center>
				<span style="font-size:20px; color:red;">COMP 6381, Fall 2021</span>
				</center>
				<center>
				<span style="font-size:20px">Digital Geometric Modelling</span>
				</center>
				</td>
			</tr>
			</tbody></table>
			
			<!--Add supplementary links here.-->
			<table align=center width=350px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Paper]</a></span>
						</center>
					</td>

					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://youtu.be/K9CrFKPUGNs'>[Video]</a></span><br>
						</center>
					</td>

					
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='resources/COMP6381_ResM3DVTON_Presentation.pdf'>[Slides]</a></span><br>
						</center>
					</td>
					
				<!-- 
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/hasibzunair/res-m3d-vton'>[GitHub]</a></span><br>
						</center>
					</td>
				-->

					




				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<!--Add intro image here.-->
						<img class="round" style="width:850px" src="./resources/combined1.gif"/>
						<img class="round" style="width:850px" src="./resources/combined2.gif"/>
					</center>
				</td>
			</tr>
			
			<!--For figure caption.
			<img class="round" style="width:800px" src="./resources/vis_3d.png"/>
			-->
			
			<tr><td width="600px">
			<center>
				<span style="font-size:14px"><i>Here are some results of our method on <strong>out-of-distribution</strong> images. Given the <strong>reference person image</strong>
					(left) and <strong>target clothing image</strong> (middle), our method can reconstruct the <strong>3D try-on mesh</strong> (right) with the clothing changed 
					 and person identity retained.
				</i>
			</span></center>
			</td>
			</tr>
			</tbody></table>

		</table>
		<table align=center width=850px>
			<tr>
				<td><br>
					<strong>TL;DR:</strong> Res-M3D-VTON is a pipeline for monocular to 3D virtual try-on (VTON) for 
					fashion clothing which uses residual learning to synthesize correct clothing parts, preserve logo of
					clothing and reduce artifacts to finally output better textured 3D try-on meshes.</a>
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				3D virtual try-on aims to synthetically fit a target clothing image onto a 3D human shape 
				while preserving realistic details such as pose, identity of the person. Existing methods heavily depend on 
				<FONT COLOR="#ff0000">annotated 3D shapes</FONT> and <FONT COLOR="#ff0000">garment templates</FONT> 
				which limits their practical use. While 2D virtual try-on is another alternative, it <FONT COLOR="#ff0000">ignores the 
				3D body information</FONT> 
				and <FONT COLOR="#ff0000">cannot fully represent the human body</FONT>. Recently, <strong><a href='https://arxiv.org/abs/2108.05126'>M3D-VTON</a></strong>
				was proposed to generate 
				textured 3D try-on meshes only from 2D images of person and clothing by formulating the 3D try-on problem
				as 2D try-on and depth estimation. However, we find that the synthesis model in the M3D-VTON pipeline 
				uses a simple U-Net architecture. We hypothesize that this is insufficient to synthesize body parts 
				and model complex relation between front and back parts of clothing only from the 2D clothing image, 
				ultimately leading to unrealistic 3D try-on results. We improve this by 
				implementing residual units in the existing synthesis model. Studying itâ€™s effect demonstrates that 
				it improves 2D try-on outputs, mainly by <FONT COLOR="#46C646">differentiating between front and back part of clothing</FONT>, 
				<FONT COLOR="#46C646">preserving logo of clothing</FONT> and <FONT COLOR="#46C646">reducing artifacts</FONT>. 
				This ultimately results in <FONT COLOR="#46C646">better textured 
				3D try-on mesh</FONT>. Benchmarking our method on the MPV3D dataset shows that it performs better than 
				previous works significantly.
				<p></p>
				<img class="round" style="width:800px" src="./resources/vis_3d.png"/>
			</td>
		</tr>

		<tr><td width="600px">
			<center>
				<span style="font-size:14px"><i>Comparison of 2D and 3D try-on mesh outputs with recent state-of-the-art 
					<a href='https://arxiv.org/abs/2108.05126'>M3D-VTON</a>.
				</i>
			</span></center>
			</td>
		</tr>

	</table>
	<hr>
	<br>

	<!--Add YouTube talk here.-->
	<!--
	<center><h1>Talk</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px"><a href=''>[Slides]</a>
				</span>
			</center>
		</tr>
	</table>
	<hr>
	-->

	<!--Add demo code here with architecture.-->
	<center><h1>Method</h1></center>
	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:1000px" src="./resources/model.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<strong>Overview of the proposed framework</strong>. Left image taken from <a href='https://arxiv.org/abs/2108.05126'>M3D-VTON paper</a>.
					
					This is an overview of the 3D virtual try-on pipeline that we build on (left). 
					We can see that there are many components involved. The major components are monocular prediction,
					depth refinement and texture fusion.
					
					<p></p>

					The monocular prediction module produces warped clothing, person segmentation and double depth maps 
					which give a base 3D shape. The depth refinement module produces the refined depth maps which 
					capture the warped clothing details as well as the high frequency details which the previous 
					module oversmooths. The texture fusion module merges the warped clothing with unchanged person 
					part to output 2D try-on results. After getting the 2D try-on and depth map, we unproject 
					the front-view and back-view depth maps to get 3D point clouds and triangulate them with 
					screened poisson reconstruction. Since the try-on image and depth maps are spatially aligned, the 
					try-on image can be used to color the front side of the mesh. As for the back texture, 
					the image is inpainting using fast marching method where the face area is filled with 
					surrounding hair color and is then mirrored to finally texture the back side of the mesh. 
					This allows us to achieve the monocular-to-3D conversion, producing the reconstructed 3D 
					try-on mesh with the clothing changed and person identity retained.

					<p></p>

					We improve the texture fusion module as it combines all the previous outputs to 
					get the final 2D try-on results and error in this step will adversely affect the final 
					textured 3D try-on mesh. We do this by implementing residual connections, shown in right.

				</td>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/hasibzunair/res-m3d-vton#running-on-custom-images'>[GitHub]</a> Demo on custom images
			</center>
		</span>
	</table>
	<br>
	<hr>

		<center><h1>Extensive results for 2D virtual try-on</h1></center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<!--Add intro image here.-->
						<img class="round" style="width:800px" src="./resources/visualization_tryons_all.png"/>
					</center>
				</td>
			</tr>
			<!--For figure caption.-->

			<tr><td width="600px">
			<center>
				<span style="font-size:14px"><i>Visual comparison of 2D try-on outputs with <a href='https://arxiv.org/abs/2108.05126'>M3D-VTON</a>.</i>
			</span></center>
			</td>
			</tr>
			</tbody></table>

		</table>
		<table align=center width=850px>
			<tr>
				<td>
					Here we show some examples of the final try-on outputs compared to previous work. 
					In many cases, we see that the baseline model is unable to <FONT COLOR="#ff0000">differentiate between the
					front and back of clothing</FONT>. It also tends to <FONT COLOR="#ff0000">change the skin color of the person</FONT>. 
					The baseline model also <FONT COLOR="#ff0000">fails to preserve the logo of clothing image</FONT>. This is due 
					to the limited capability of the U-Net architecture employed in the baseline model. In comparison, 
					the proposed method generates realistic try-on results which <strong>differentiates front and 
					back part of clothing</strong>, <strong> preserve logo of clothing</strong>. It also <strong>reduces 
					artifacts</strong> in non-target body parts such as skin.
</a>
				</td>
			</tr>
		</table>
		<br>
		<hr>


	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">H. Zunair<br>
				<b>Monocular-to-3D Virtual Try-On using Deep Residual U-Net.</b><br>
				COMP 6321 Digital Geometric Modelling Course Project, Fall 2021.<br>
				<!--Add arXiv link-->
				<!--(hosted on <a href="">ArXiv</a>)<br> -->
				<!--Add camera ready link-->
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<!--Add Bibtex-->
	<!--
	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>
	-->
	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					We thank <a href="https://users.encs.concordia.ca/~stpopa/">Dr. Tiberiu Popa</a> for useful discussions during the development of this project. 
					We also thank Concordia University and Compute Canada for providing computational resources and support that contributed to these research results.
					Some of the computing for this project was performed on the Graham cluster. This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

